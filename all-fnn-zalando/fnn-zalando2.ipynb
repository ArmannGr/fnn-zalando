{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FNN Zalando\n",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:27.540705Z",
     "start_time": "2025-01-06T11:58:27.506500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy\n",
    "# scipy.special für die Sigmoid-Funktion expit()\n",
    "import scipy.special\n",
    "# Bibliothek zum Plotten von Arrays\n",
    "import matplotlib.pyplot\n",
    "# sicherstellen, dass die Plots in diesem Notebook und nicht in einem externen Fenster angezeigt werden\n",
    "%matplotlib inline"
   ],
   "id": "bc6c430c935c4b95",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "511fa870da5dfe0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:27.579677Z",
     "start_time": "2025-01-06T11:58:27.563824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Definition der neuronalen Netzwerkklasse\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # Initialisierung des neuronalen Netzwerks\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # Anzahl der Nodes in jeder Input-, hidden und output layer festlegen\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # Gewichtsmatrizen, wih und who\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # Lernrate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # Aktivierungsfunktion ist die Sigmoid-Funktion\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # Trainieren des neuronalen Netzwerks\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # Inputs in ein 2D-Array umwandeln\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # Signale in dem Hidden Layer berechnen\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # Signale berechnen, die aus dem Hidden Layer hervorgehen\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # Signale in dem endgültigen Output Layer berechnen\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # Signale berechnen, die aus dem endgültigen Output Layer  hervorgehen\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # Fehler in dem Output Layer ist (Ziel - Ist)\n",
    "        output_errors = targets - final_outputs\n",
    "        # Fehler in dem Hidden Layer ist der output_errors, aufgeteilt nach Gewichten, neu kombiniert an den Hidden Nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # Gewichte für die Verbindungen zwischen den hidden und den Output Layers aktualisieren\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # Gewichte für die Verbindungen zwischen den Input- und den Hidden Layern aktualisieren\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # Abfragen des neuronalen Netzwerks\n",
    "    def query(self, inputs_list):\n",
    "        # Input List in ein 2D-Array umwandeln\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # Signale in die Hidden Layern berechnen\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # Signale berechnen, die aus dem Hidden Layer hervorgehen\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # Signale in den endgültigen Output Layer berechnen\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # Signale berechnen, die aus dem endgültigen Output Layer hervorgehen\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:27.683626Z",
     "start_time": "2025-01-06T11:58:27.668372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Anzahl an input, hidden und output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 1\n",
    "\n",
    "# Lernrate\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Erstelle Instanz des neuronalen Netzwerks\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)"
   ],
   "id": "58d6204bf1f84c28",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:28.177247Z",
     "start_time": "2025-01-06T11:58:27.734128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/fashion-mnist_train.csv\", 'r') as training_data_file:\n",
    "    training_data_list = training_data_file.readlines()[1:]"
   ],
   "id": "2361e8c8b73646b1",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:44.857700Z",
     "start_time": "2025-01-06T11:58:28.304516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Epochen ist die Anzahl der Male, die der Trainingsdatensatz zum Training verwendet wird\n",
    "epochs = 1\n",
    "\n",
    "# Zähler für übersprungene Datensätze\n",
    "skipped_records = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    # Gehe durch alle Datensätze im Trainingsdatensatz\n",
    "    for record in training_data_list:\n",
    "        # Teile den Datensatz durch die ',' Kommas\n",
    "        all_values = record.split(',')\n",
    "        # Überprüfe, ob alle Werte vorhanden sind\n",
    "        if len(all_values) < 2 or '' in all_values:\n",
    "            skipped_records += 1\n",
    "            continue\n",
    "        # Skaliere und verschiebe die Eingaben\n",
    "        inputs = (numpy.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "        # Erstelle den Zielausgabewert (0.01 oder 0.99)\n",
    "        target = numpy.array([0.99 if int(all_values[0]) == 1 else 0.01])\n"
   ],
   "id": "9db035926e48478f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:44.875716Z",
     "start_time": "2025-01-06T11:58:44.872443Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Skipped records: {skipped_records}/{len(training_data_list)}')",
   "id": "628fd1e75b654642",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped records: 0/60000\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6d1feb6be662aca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:44.966575Z",
     "start_time": "2025-01-06T11:58:44.923006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/fashion-mnist_test.csv\", 'r') as test_data_file:\n",
    "    test_data_list = test_data_file.readlines()[1:]"
   ],
   "id": "2d0232118498a7b",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:45.970396Z",
     "start_time": "2025-01-06T11:58:44.978784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testen des neuronalen Netzwerks\n",
    "\n",
    "# Scorecard für die Leistung des Netzwerks, anfangs leer\n",
    "scorecard = []\n",
    "\n",
    "# Zähler für übersprungene Datensätze\n",
    "skipped_records = 0\n",
    "\n",
    "# Gehe durch alle Datensätze im Testdatensatz\n",
    "for record in test_data_list:\n",
    "    # Teile den Datensatz durch die ',' Kommas\n",
    "    all_values = record.split(',')\n",
    "    # Überprüfe, ob alle Werte vorhanden sind\n",
    "    if len(all_values) != 785:  # 1 Label + 784 Merkmale\n",
    "        skipped_records += 1\n",
    "        continue\n",
    "    # Korrekte Antwort ist der erste Wert\n",
    "    correct_label = int(all_values[0])\n",
    "    # Skaliere und verschiebe die Eingaben\n",
    "    inputs = (numpy.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "    # Abfrage des Netzwerks\n",
    "    outputs = n.query(inputs)\n",
    "    # Der Output ist ein einzelner Wert, daher vergleichen wir ihn direkt\n",
    "    predicted_label = 1 if outputs[0] > 0.5 else 0\n",
    "    # Füge korrekt oder inkorrekt zur Liste hinzu\n",
    "    if predicted_label == correct_label:\n",
    "        # Antwort des Netzwerks stimmt mit der korrekten Antwort überein, füge 1 zur Scorecard hinzu\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # Antwort des Netzwerks stimmt nicht mit der korrekten Antwort überein, füge 0 zur Scorecard hinzu\n",
    "        scorecard.append(0)"
   ],
   "id": "5fc9a74f88a5883e",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:45.986524Z",
     "start_time": "2025-01-06T11:58:45.983878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Berechnen Sie die Leistung, die das Verhältnis der richtigen Antworten ist\n",
    "performance = sum(scorecard) / len(scorecard)\n",
    "print(f'Performance: {performance}')\n",
    "print(f'Skipped records: {skipped_records}/{len(test_data_list)}')"
   ],
   "id": "a2cb8b6ee1fee220",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 0.1\n",
      "Skipped records: 0/10000\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:58:46.050681Z",
     "start_time": "2025-01-06T11:58:46.015727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Erstellen Sie eine Liste von 10 Modellen\n",
    "models = []\n",
    "for i in range(10):\n",
    "    models.append(neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate))"
   ],
   "id": "37c33dd52dbcf1b8",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:05:27.565650Z",
     "start_time": "2025-01-06T11:58:46.062347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Epochen ist die Anzahl der Male, die der Trainingsdatensatz zum Training verwendet wird\n",
    "epochs = 3\n",
    "\n",
    "# Zähler für übersprungene Datensätze\n",
    "skipped_records = 0\n",
    "\n",
    "# Trainieren Sie jedes Modell auf die jeweilige Klasse gegen den Rest\n",
    "for i in range(10):\n",
    "    for e in range(epochs):\n",
    "        for record in training_data_list:\n",
    "            all_values = record.split(',')\n",
    "            if len(all_values) < 2 or '' in all_values:\n",
    "                skipped_records += 1\n",
    "                continue\n",
    "            inputs = (numpy.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "            target = numpy.array([0.99 if int(all_values[0]) == i else 0.01])\n",
    "            models[i].train(inputs, target)"
   ],
   "id": "7f0e7eb9227ee741",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:05:29.886861Z",
     "start_time": "2025-01-06T12:05:27.577219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testen Sie jedes Bild mit allen 10 Modellen\n",
    "scorecard = []\n",
    "for record in test_data_list:\n",
    "    all_values = record.split(',')\n",
    "    if len(all_values) != 785:\n",
    "        skipped_records += 1\n",
    "        continue\n",
    "    correct_label = int(all_values[0])\n",
    "    inputs = (numpy.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "    outputs = [model.query(inputs) for model in models]\n",
    "    predicted_labels = [i for i, output in enumerate(outputs) if output[0] > 0.5]\n",
    "    if correct_label in predicted_labels:\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)"
   ],
   "id": "2ef4498a59f7ffab",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:05:29.908557Z",
     "start_time": "2025-01-06T12:05:29.905456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Berechnen Sie die Leistung\n",
    "performance = sum(scorecard) / len(scorecard)\n",
    "print(f'Performance: {performance}')\n",
    "print(f'Skipped records: {skipped_records}/{len(test_data_list)}')"
   ],
   "id": "1b5e0ae64ba9c8f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 0.8393\n",
      "Skipped records: 0/10000\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc3ca44304a1991d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "267e631101f19c2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:05:30.260039Z",
     "start_time": "2025-01-06T12:05:29.935158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Erstellen Sie eine Liste von 10 Ensembles, die jeweils 10 Modelle enthalten\n",
    "ensembles = []\n",
    "for _ in range(10):\n",
    "    ensemble = []\n",
    "    for i in range(10):\n",
    "        ensemble.append(neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate))\n",
    "    ensembles.append(ensemble)"
   ],
   "id": "89f8cc7a3d1743c0",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:29:46.456154Z",
     "start_time": "2025-01-06T12:05:30.274173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Trainiere jedes Modell in jedem Ensemble auf die jeweilige Klasse gegen den Rest\n",
    "for ensemble in ensembles:\n",
    "    for i in range(10):\n",
    "        for e in range(epochs):\n",
    "            for record in training_data_list:\n",
    "                all_values = record.split(',')\n",
    "                if len(all_values) < 2 or '' in all_values:\n",
    "                    skipped_records += 1\n",
    "                    continue\n",
    "                inputs = (numpy.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "                target = numpy.array([0.99 if int(all_values[0]) == i else 0.01])\n",
    "                ensemble[i].train(inputs, target)"
   ],
   "id": "a6e59fc0b528fc69",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:30:22.267555Z",
     "start_time": "2025-01-06T13:29:46.493930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testen Sie jedes Bild mit allen Modellen in allen Ensembles\n",
    "scorecard = []\n",
    "for record in test_data_list:\n",
    "    all_values = record.split(',')\n",
    "    if len(all_values) != 785:\n",
    "        skipped_records += 1\n",
    "        continue\n",
    "    correct_label = int(all_values[0])\n",
    "    inputs = (numpy.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "    ensemble_outputs = []\n",
    "    for ensemble in ensembles:\n",
    "        outputs = [model.query(inputs) for model in ensemble]\n",
    "        predicted_labels = [i for i, output in enumerate(outputs) if output[0] > 0.5]\n",
    "        ensemble_outputs.append(predicted_labels)\n",
    "    # Aggregieren Sie die Vorhersagen aller Ensembles\n",
    "    all_predictions = sum(ensemble_outputs, [])\n",
    "    if all_predictions:\n",
    "        final_prediction = max(set(all_predictions), key=all_predictions.count)\n",
    "        if correct_label == final_prediction:\n",
    "            scorecard.append(1)\n",
    "        else:\n",
    "            scorecard.append(0)\n",
    "    else:\n",
    "        # Behandeln Sie den Fall, in dem kein Modell ein Label mit einer Konfidenz > 0.5 vorhersagt\n",
    "        scorecard.append(0)"
   ],
   "id": "a56093d46963804a",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:30:22.347422Z",
     "start_time": "2025-01-06T13:30:22.344883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Berechne die Leistung\n",
    "performance = sum(scorecard) / len(scorecard)\n",
    "print(f'Performance: {performance}')\n",
    "print(f'Skipped records: {skipped_records}/{len(test_data_list)}')"
   ],
   "id": "7402cd41bf648ab9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 0.8425\n",
      "Skipped records: 0/10000\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:02:43.700914Z",
     "start_time": "2025-01-06T14:02:43.117228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funktion zur Berechnung der Sicherheitsmaße\n",
    "def calculate_confidence_scores(data_list, ensembles):\n",
    "    confidence_scores = []\n",
    "    for record in data_list:\n",
    "        all_values = record.split(',')\n",
    "        if len(all_values) != 785:\n",
    "            continue\n",
    "        inputs = (np.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "        ensemble_outputs = []\n",
    "        for ensemble in ensembles:\n",
    "            outputs = [model.query(inputs) for model in ensemble]\n",
    "            predicted_labels = [i for i, output in enumerate(outputs) if output[0] > 0.5]\n",
    "            ensemble_outputs.append(predicted_labels)\n",
    "        all_predictions = sum(ensemble_outputs, [])\n",
    "        if all_predictions:\n",
    "            confidence_score = max(set(all_predictions), key=all_predictions.count)\n",
    "            confidence_scores.append(confidence_score)\n",
    "    return confidence_scores\n",
    "\n",
    "# Berechne die Sicherheitsmaße für Trainings- und Testdaten\n",
    "train_confidence_scores = calculate_confidence_scores(training_data_list, ensembles)\n",
    "test_confidence_scores = calculate_confidence_scores(test_data_list, ensembles)\n",
    "\n",
    "# Erstelle das Histogramm\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(train_confidence_scores, bins=50, alpha=0.5, label='Trainingsdaten')\n",
    "plt.hist(test_confidence_scores, bins=50, alpha=0.5, label='Testdaten')\n",
    "plt.xlabel('Sicherheitsmaß')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.title('Verteilung der Sicherheitsmaßergebnisse für Trainings- und Testbilder')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ],
   "id": "651d21f3d65e520b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m confidence_scores\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Berechne die Sicherheitsmaße für Trainings- und Testdaten\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m train_confidence_scores \u001B[38;5;241m=\u001B[39m calculate_confidence_scores(\u001B[43mtraining_data_list\u001B[49m, ensembles)\n\u001B[1;32m     25\u001B[0m test_confidence_scores \u001B[38;5;241m=\u001B[39m calculate_confidence_scores(test_data_list, ensembles)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Erstelle das Histogramm\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'training_data_list' is not defined"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
